{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the tree methods：\n",
    "\n",
    "Sometimes I play tennis every Saturday and I always invite a friend to come with me.Sometimes my friends shows up, sometimes not.\n",
    "\n",
    "For him it depends on a variety of factors, such as: weather,temperature,humidity,wind...\n",
    "\n",
    "I start keeping track of these features and whether or not he showed up to play with me.\n",
    "\n",
    "So can use this features to predict whether or not he will show up to play, an intuitive way to do this is through a decision tree.\n",
    "___\n",
    "#### What is  decision tree ?\n",
    "\n",
    "1. Nodes： Split for the value of a certain attribute.\n",
    "\n",
    "2. Edges： Outcome of a split to next node.\n",
    "\n",
    "3. Roots： The node that performs the first split.\n",
    "\n",
    "4. Leaves： Terminal nodes that predict the outcome.\n",
    "\n",
    "5. Use Entropy and information Gain, both of the methods are the math methods of choosing the best split.\n",
    "\n",
    "6. The primary weakness of decision tree is that it can't tend to have the best predictive accuracy.\n",
    "\n",
    "7. It is due to the high variance meaning that different splits in the training data can lead to very different trees.\n",
    "___\n",
    "#### What is Random Forests？\n",
    "\n",
    "It is a way to improve performance off single decision trees.\n",
    "\n",
    "1. It is a slight variation of trees that has even better performance.\n",
    "\n",
    "2. A forest is built in a random way, there are many decision trees in the forest, and no correlation between each decision tree in the random forest.\n",
    "\n",
    "3. Create an ensembel of decision trees using bootstrap samples of the training set, it just means sampling from the training set with replacement.\n",
    "\n",
    "4. Building each tree each time ,a split is considered a random sample of M features is chosen as a split candidate from the full set of p features.\n",
    "\n",
    "5. The split is only allowed to use one of those m features, a new random sample of features is chosen for every single tree, every single split.\n",
    "\n",
    "6. For classfication, this m random sample of M features is typically chosen to the square root of p where p is the full set of features.\n",
    "___\n",
    "#### What is Random Forests point？\n",
    "\n",
    "1. Suppose there is one very strong feature in the data set. When using 'bagged' trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are highly correlated.\n",
    "\n",
    "2. Averaging highly correlated quantities does not sighificantly reduce variance.\n",
    "\n",
    "3. By randomly leaving out candidate features from each split, random forests decorrelates the trees(making the trees inpedent of each other), such that the averaging process can reduce the variance of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
